{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summarization_with_Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "fA-7yjndDpIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    data set file: https://drive.google.com/file/d/13q7pqpx-a8QIyRLXYAnLNJuFWyqcw-2b/view?usp=sharing\n",
        "    dowload and place in drive (My drive folder)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7bNPab5ywWmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "biTcBJi7-JUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "hgAMd_kSr1yW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b master https://github.com/vohung471999/Transformer.git"
      ],
      "metadata": {
        "id": "eEIs7tuFrzCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import copy\n",
        "import math\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import gc"
      ],
      "metadata": {
        "id": "rDqOu9uybCLA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "S2zmurHU0EJj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tranformer\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K9YDHct4Jbjy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbIJs0PDAVzD"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class WordEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, model_dim, padding_idx):\n",
        "        super().__init__()\n",
        "        kwargs = {'device': device, 'dtype': torch.float32}\n",
        "        self.vocab_size = vocab_size\n",
        "        self.model_dim = model_dim\n",
        "        self.embed = nn.Embedding(vocab_size, model_dim, padding_idx=padding_idx, **kwargs)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.embed(x)\n",
        "\n",
        "class NormalPositionalEmbedding(nn.Embedding):\n",
        "\n",
        "    def __init__(self, embedding_dim: int, num_embeddings=1024):\n",
        "        kwargs = {'device': device, 'dtype': torch.float32}\n",
        "        self.offset = 2\n",
        "        super().__init__(num_embeddings + self.offset, embedding_dim, **kwargs)\n",
        "\n",
        "    def forward(self, input_ids_shape: torch.Size):\n",
        "        bsz, seq_len = input_ids_shape[:2]\n",
        "        positions = torch.arange(0, seq_len, dtype=torch.long, device=self.weight.device)\n",
        "        return super().forward(positions + self.offset)\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, model_dim, dropout=0.1, max_seq_len=1024):\n",
        "        super().__init__()\n",
        "        self.model_dim = model_dim\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        positional_emb = torch.zeros(max_seq_len, model_dim)\n",
        "        position = torch.arange(0, max_seq_len).unsqueeze(1)\n",
        "        w = torch.exp(torch.arange(0, model_dim, 2)*(-math.log(10000)/model_dim))\n",
        "\n",
        "        positional_emb[:, 0::2] = torch.sin(position * w)\n",
        "        positional_emb[:, 1::2] = torch.cos(position * w)\n",
        "\n",
        "        positional_emb = positional_emb.unsqueeze(0)\n",
        "        self.register_buffer('positional_emb', positional_emb)\n",
        "\n",
        "    def forward(self, embedding):\n",
        "        embedding = embedding*math.sqrt(self.model_dim)\n",
        "        seq_len = embedding.size(1)\n",
        "        \n",
        "        positional_emb = Variable(self.positional_emb[:, :seq_len], requires_grad=False)\n",
        "        embedding = embedding + positional_emb\n",
        "\n",
        "        return self.dropout(embedding)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9-mNGQZZQs-"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, embed_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        kwargs = {'device': device, 'dtype': torch.float32}\n",
        "        assert embed_dim % num_heads == 0\n",
        "        \n",
        "        self.num_heads = num_heads\n",
        "        self.embed_dim = embed_dim\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.attention_weight = None\n",
        "\n",
        "        self.query_project = nn.Linear(embed_dim, embed_dim, **kwargs)\n",
        "        self.key_project = nn.Linear(embed_dim, embed_dim, **kwargs)\n",
        "        self.value_project = nn.Linear(embed_dim, embed_dim, **kwargs)\n",
        "        self.out_matrix = nn.Linear(embed_dim, embed_dim, **kwargs)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def _self_attention(self, query, key, value, attention_mask=None, dropout=None):\n",
        "        \"\"\"\n",
        "        q: batch_size x heads x seq_length x d_model\n",
        "        k: batch_size x heads x seq_length x d_model\n",
        "        v: batch_size x heads x seq_length x d_model\n",
        "        attention_mask: batch_size x 1 x seq_length\n",
        "        output: batch_size x head x seq_length x d_model\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, num_of_heads, seq_length, dim_head = query.shape\n",
        "        attention_scores = torch.matmul(query, key.transpose(-1, -2)) * (dim_head ** -0.5)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_scores = attention_scores.masked_fill(attention_mask==0, float('-inf'))\n",
        "\n",
        "        attention_scores = nn.functional.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "\n",
        "        attention_output = torch.matmul(attention_scores, value)\n",
        "        return attention_output, attention_scores\n",
        "    \n",
        "    def _shape(self, tensor: torch.Tensor, sequence_length: int, batch_size: int):\n",
        "        return tensor.view(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
        "\n",
        "    def forward(self, \n",
        "                query, \n",
        "                key, \n",
        "                value,\n",
        "                attention_mask = None):\n",
        "        \n",
        "        batch_size, tgt_length, _ = query.shape\n",
        "        _, src_length, _ = key.shape\n",
        "\n",
        "        q = self.query_project(query)\n",
        "        k = self.key_project(key)\n",
        "        v = self.value_project(value)\n",
        "\n",
        "        # change shape to (batch_size, number_of_heads, sequence_length, dim_head)\n",
        "        q = q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        attention_output, self.attention_weight = self._self_attention(q, k, v, attention_mask, self.dropout)\n",
        "\n",
        "        attention_output =  attention_output.transpose(1, 2).contiguous()\n",
        "        attention_output =  attention_output.view(batch_size, tgt_length, self.embed_dim)\n",
        "        attention_output = self.out_matrix(attention_output)\n",
        "\n",
        "        return attention_output\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fN1WotBaL1p"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        kwargs = {'device': device, 'dtype': torch.float32}\n",
        "\n",
        "        self.attention = MultiHeadAttention(num_heads, embed_dim, dropout=dropout)\n",
        "        self.attention_dropout = nn.Dropout(dropout)\n",
        "        self.attention_norm = nn.LayerNorm(embed_dim, eps=1e-5, **kwargs)\n",
        "\n",
        "        self.linear_1 = nn.Linear(embed_dim, 4096, **kwargs)\n",
        "        self.activation = F.gelu\n",
        "        self.activation_dropout = nn.Dropout(0.0)\n",
        "\n",
        "        self.linear_2 = nn.Linear(4096, embed_dim, **kwargs)\n",
        "        self.ff_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.ff_norm = nn.LayerNorm(embed_dim, eps=1e-5, **kwargs)\n",
        "        \n",
        "\n",
        "    def forward(self, hidden_states, encoder_attention_mask):\n",
        "\n",
        "        # attention block\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.attention(hidden_states, hidden_states, hidden_states, encoder_attention_mask)\n",
        "        hidden_states = self.attention_dropout(hidden_states)\n",
        "\n",
        "        # residual + normalization block\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.attention_norm(hidden_states)\n",
        "\n",
        "        # feed forward block\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.linear_1(hidden_states)\n",
        "        hidden_states = self.activation(hidden_states)\n",
        "        hidden_states = self.activation_dropout(hidden_states)\n",
        "        hidden_states = self.linear_2(hidden_states)\n",
        "        hidden_states = self.ff_dropout(hidden_states)\n",
        "\n",
        "        # residual + norm block\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.ff_norm(hidden_states)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_encoder_layers, num_heads, dropout, embed_tokens):\n",
        "        super().__init__()\n",
        "        kwargs = {'device': device, 'dtype': torch.float32}\n",
        "        self.num_encoder_layers = num_encoder_layers\n",
        "        self.word_embedding = embed_tokens\n",
        "        self.positional_embedding = NormalPositionalEmbedding(embed_dim, 1024)\n",
        "        # self.positional_embedding = PositionalEmbedding(embed_dim, dropout=dropout)\n",
        "        self.norm_embedding = nn.LayerNorm(embed_dim, **kwargs)\n",
        "        self.layers = nn.ModuleList([copy.deepcopy(EncoderLayer(embed_dim, num_heads, dropout)) for _ in range(self.num_encoder_layers)])\n",
        "\n",
        "    def forward(self, encoder_inputs, encoder_attention_mask):\n",
        "        input_shape = encoder_inputs.size()\n",
        "\n",
        "        # # embedding layer\n",
        "        # word_embed = self.word_embedding(encoder_inputs)\n",
        "        # hidden_states = self.positional_embedding(word_embed)\n",
        "        # hidden_states = self.norm_embedding(hidden_states)\n",
        "\n",
        "        # embedding layer\n",
        "        word_embed = self.word_embedding(encoder_inputs)\n",
        "        pos_embed = self.positional_embedding(input_shape)\n",
        "        hidden_states = word_embed + pos_embed\n",
        "        hidden_states = self.norm_embedding(hidden_states)\n",
        "\n",
        "        #encoder layers\n",
        "        for layer in self.layers:\n",
        "            hidden_states = layer(hidden_states, encoder_attention_mask=encoder_attention_mask)\n",
        "\n",
        "        return hidden_states\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAOVJaKWaL5N"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        kwargs = {'device': device, 'dtype': torch.float32}\n",
        "\n",
        "        self.self_attention = MultiHeadAttention(num_heads, embed_dim, dropout=dropout)\n",
        "        self.self_attention_dropout = nn.Dropout(dropout)\n",
        "        self.self_attention_norm =  nn.LayerNorm(embed_dim, eps=1e-5, **kwargs)\n",
        "\n",
        "        self.cross_attention = MultiHeadAttention(num_heads, embed_dim, dropout=dropout)\n",
        "        self.cross_attention_dropout = nn.Dropout(dropout)\n",
        "        self.cross_attention_norm = nn.LayerNorm(embed_dim, eps=1e-5, **kwargs)\n",
        "\n",
        "        self.linear_1 = nn.Linear(embed_dim, 4096, **kwargs)\n",
        "        self.activation = F.gelu\n",
        "        self.activation_dropout = nn.Dropout(0.0)\n",
        "        self.linear_2 = nn.Linear(4096, embed_dim, **kwargs)\n",
        "        self.ff_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.ff_norm = nn.LayerNorm(embed_dim, eps=1e-5, **kwargs)\n",
        "\n",
        "    def forward(self, hidden_states, encoder_outputs, decoder_self_attention_mask, decoder_cross_attention_mask):            \n",
        "        # self attention block\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.self_attention(hidden_states, hidden_states, hidden_states, decoder_self_attention_mask)\n",
        "        hidden_states = self.self_attention_dropout(hidden_states)\n",
        "\n",
        "        # residual + normalization block\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.self_attention_norm(hidden_states)\n",
        "\n",
        "        # cross attention block\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.cross_attention(hidden_states, encoder_outputs, encoder_outputs, decoder_cross_attention_mask)\n",
        "        hidden_states = self.cross_attention_dropout(hidden_states)\n",
        "\n",
        "        # residual + normalization block\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.cross_attention_norm(hidden_states)\n",
        "\n",
        "        # feed forward block\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.linear_1(hidden_states)\n",
        "        hidden_states = self.activation(hidden_states)\n",
        "        hidden_states = self.activation_dropout(hidden_states)\n",
        "        hidden_states = self.linear_2(hidden_states)\n",
        "        hidden_states = self.ff_dropout(hidden_states)\n",
        "\n",
        "        # residual + norm block\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.ff_norm(hidden_states)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_decoder_layers, num_heads, dropout, embed_tokens):\n",
        "        super().__init__()\n",
        "        kwargs = {'device': device, 'dtype': torch.float32}\n",
        "        self.num_decoder_layers = num_decoder_layers\n",
        "        self.word_embedding = embed_tokens\n",
        "        self.positional_embedding = NormalPositionalEmbedding(embed_dim, 1024)\n",
        "        # self.positional_embedding = PositionalEmbedding(embed_dim, dropout=dropout)\n",
        "        self.norm_embedding = nn.LayerNorm(embed_dim, eps=1e-5, **kwargs)\n",
        "        self.layers = nn.ModuleList([copy.deepcopy(DecoderLayer(embed_dim, num_heads, dropout)) for _ in range(self.num_decoder_layers)])\n",
        "\n",
        "    def forward(self, decoder_input, encoder_hidden_states, decoder_self_attention_mask, decoder_cross_attention_mask):\n",
        "        input_shape = decoder_input.size()\n",
        "\n",
        "        # # embedding layer\n",
        "        # word_embed = self.word_embedding(decoder_input)\n",
        "        # hidden_states = self.positional_embedding(word_embed)\n",
        "        # hidden_states = self.norm_embedding(hidden_states)\n",
        "\n",
        "        # embedding layer\n",
        "        word_embed = self.word_embedding(decoder_input)\n",
        "        pos_embed = self.positional_embedding(input_shape)\n",
        "        hidden_states = word_embed + pos_embed\n",
        "        hidden_states = self.norm_embedding(hidden_states)\n",
        "\n",
        "        #decoder layers\n",
        "        for layer in self.layers:\n",
        "            hidden_states = layer(hidden_states, encoder_hidden_states, decoder_self_attention_mask, decoder_cross_attention_mask)\n",
        "            \n",
        "        return hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_of_encoder_decoder, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        kwargs = {'device': device, 'dtype': torch.float32}\n",
        "        self.padding_idx = 1 \n",
        "        self.word_embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=self.padding_idx, **kwargs)\n",
        "        self.encoder = Encoder(embed_dim, num_of_encoder_decoder, num_heads, dropout, self.word_embedding)\n",
        "        self.decoder = Decoder(embed_dim, num_of_encoder_decoder, num_heads, dropout, self.word_embedding)\n",
        "        self.final_output = nn.Linear(embed_dim, vocab_size, **kwargs)\n",
        "\n",
        "    def forward(self, encoder_inputs, decoder_inputs, encoder_attention_mask, decoder_self_attention_mask):\n",
        "\n",
        "        encoder_hidden_states = self.encoder(encoder_inputs, encoder_attention_mask)\n",
        "        final_hidden_states = self.decoder(decoder_inputs, encoder_hidden_states,  decoder_self_attention_mask, encoder_attention_mask)\n",
        "\n",
        "        output = self.final_output(final_hidden_states)\n",
        "        return output\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n"
      ],
      "metadata": {
        "id": "0LG1UXqoKuan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Init Transformer from github"
      ],
      "metadata": {
        "id": "mjXvYrYqsGkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Transformer.utils.beamsearch import beam_summarize\n",
        "from Transformer.model.transformer_model import Transformer\n",
        "from Transformer.model.transformer_config import TransformerConfig\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "import torch\n",
        "\n",
        "kwargs = {\n",
        "    'vocab_size': 50264,\n",
        "    'max_position_embeddings': 1024,\n",
        "    'num_encoder_layers': 12,\n",
        "    'encoder_ffn_dim': 4096,\n",
        "    'encoder_attention_heads': 16,\n",
        "    'num_decoder_layers': 12,\n",
        "    'decoder_ffn_dim': 4096,\n",
        "    'decoder_attention_heads': 16,\n",
        "    'encoder_layer_dropout': 0.0,\n",
        "    'decoder_layer_dropout': 0.0,\n",
        "    'activation_function': 'gelu',\n",
        "    'layer_norm_eps': 1e-5,\n",
        "    'model_dim': 1024,\n",
        "    'dropout': 0.1,\n",
        "    'attention_dropout': 0.0,\n",
        "    'activation_dropout': 0.0,\n",
        "    'pad_token_id': 1,\n",
        "    'device': device,\n",
        "    'dtype': torch.float32\n",
        "}\n",
        "tran_conf = TransformerConfig(**kwargs)\n",
        "\n",
        "transformer = Transformer(tran_conf)\n",
        "\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')"
      ],
      "metadata": {
        "id": "borONqoksNtg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilies"
      ],
      "metadata": {
        "id": "0pKALhHCJ5yM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(sequence, padding_token, device):\n",
        "    batch_size, inputs_len = sequence.size()\n",
        "    mask = (sequence != padding_token)\n",
        "    mask = mask[:, None, None, :].expand(batch_size, 1, 1, inputs_len)\n",
        "    mask = mask.to(device)\n",
        "    return mask\n",
        "\n",
        "def create_casual_mask(sequence, device):\n",
        "    batch_size, input_len = sequence.size()\n",
        "    casual_mask = np.triu(np.ones((batch_size, input_len, input_len)), k=1).astype('uint8')\n",
        "    casual_mask =  Variable(torch.from_numpy(casual_mask) == 0)\n",
        "    casual_mask = casual_mask.unsqueeze(1)\n",
        "    casual_mask = casual_mask.to(device)\n",
        "    return casual_mask\n",
        "\n",
        "def create_mask(encoder_inputs, decoder_inputs, padding_token, device):\n",
        "    encoder_attention_mask = create_padding_mask(encoder_inputs, padding_token, device)\n",
        "    \n",
        "    decoder_padding_mask = create_padding_mask(decoder_inputs, padding_token, device)\n",
        "    decoder_casual_mask = create_casual_mask(decoder_inputs, device)\n",
        "\n",
        "    decoder_self_attention_mask = decoder_casual_mask.logical_and(decoder_padding_mask)\n",
        "\n",
        "    return encoder_attention_mask, decoder_self_attention_mask\n"
      ],
      "metadata": {
        "id": "Xplk-GnkNxGi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beam Search"
      ],
      "metadata": {
        "id": "TO4Ley8otIbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_beam(model, text, text_mask, max_seq_len, start_token_id, num_beams, device):\n",
        "    text = text.to(device)\n",
        "    text_mask = text_mask.to(device)\n",
        "\n",
        "    memory_state = model.encoder(text, text_mask)\n",
        "    batch_size, text_length, model_dim = memory_state.shape\n",
        "\n",
        "    summary = torch.LongTensor([[start_token_id]]).to(device)\n",
        "    summary_casual_mask = create_casual_mask(summary, device)\n",
        "    summary_padding_mask = create_padding_mask(summary, 1, device)\n",
        "    summary_mask = summary_casual_mask.logical_and(summary_padding_mask)\n",
        "\n",
        "    model_outputs = model.final_output(model.decoder(summary, memory_state, summary_mask, text_mask))\n",
        "    log_scores, index = F.log_softmax(model_outputs, dim=-1).topk(num_beams)\n",
        "\n",
        "    model_outputs = torch.zeros((num_beams, max_seq_len), dtype=torch.int32, device=device)\n",
        "    model_outputs[:, 0] = start_token_id\n",
        "    model_outputs[:, 1] = index[0]\n",
        "    memory_state = memory_state.expand(num_beams, text_length, model_dim)\n",
        "\n",
        "    return model_outputs, log_scores, memory_state"
      ],
      "metadata": {
        "id": "Ju6s4LR88dsA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_k_top_candidate(model_outputs, prob, log_scores, i, num_beams):\n",
        "    log_outputs = F.log_softmax(prob, dim=-1)\n",
        "    \n",
        "    log_probs, index = log_outputs[:, -1].topk(num_beams)\n",
        "    log_probs = log_probs + log_scores.transpose(0, 1)\n",
        "    log_probs, k_index = log_probs.view(-1).topk(num_beams)\n",
        "\n",
        "    rows = torch.div(k_index, num_beams, rounding_mode='floor')\n",
        "    cols = k_index % num_beams\n",
        "    model_outputs[:, :i] = model_outputs[rows, :i]\n",
        "    model_outputs[:, i] = index[rows, cols]\n",
        "    \n",
        "    log_scores = log_probs.unsqueeze(0)\n",
        "\n",
        "    return model_outputs, log_scores"
      ],
      "metadata": {
        "id": "caEdHngy7ZV6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam(model, text, text_mask, max_seq_len, start_token_id, end_token_id, num_beams, device):\n",
        "\n",
        "    max_seq_len = 1024 if max_seq_len > 1024 else max_seq_len\n",
        "    chosen_text_index = 0\n",
        "    model_outputs, log_scores, memory_state = init_beam(model, text, text_mask, max_seq_len, start_token_id, num_beams, device)\n",
        "\n",
        "    for i in range(2, max_seq_len):\n",
        "        summary_casual_mask = create_casual_mask(model_outputs[:, :i], device)\n",
        "        summary_padding_mask = create_padding_mask(model_outputs[:, :i], 1, device)\n",
        "        summary_mask = summary_casual_mask.logical_and(summary_padding_mask)\n",
        "        \n",
        "        prob = model.final_output(model.decoder(model_outputs[:, :i], memory_state, summary_mask, text_mask))\n",
        "        model_outputs, log_scores = select_k_top_candidate(model_outputs, prob, log_scores, i, num_beams)\n",
        "\n",
        "        finished_sentences = (model_outputs == end_token_id).nonzero()\n",
        "        mark_end_tokens = torch.zeros(num_beams, dtype=torch.int64, device=device)\n",
        "        num_finished_sentences = 0\n",
        "\n",
        "        for end_token in finished_sentences:\n",
        "            sentence_ind, end_token_location = end_token\n",
        "            if mark_end_tokens[sentence_ind] == 0:\n",
        "                mark_end_tokens[sentence_ind] = end_token_location\n",
        "                num_finished_sentences += 1\n",
        "    \n",
        "        if num_finished_sentences == num_beams:\n",
        "            alpha = 0.7\n",
        "            division = mark_end_tokens.type_as(log_scores)**alpha\n",
        "            _, chosen_text_index = torch.max(log_scores / division, 1)\n",
        "            chosen_text_index = chosen_text_index[0]\n",
        "            break\n",
        "  \n",
        "    text_length = (model_outputs[chosen_text_index] == end_token_id).nonzero()\n",
        "    text_length = text_length[0] if len(text_length) > 0 else -1\n",
        "    return model_outputs[chosen_text_index][:text_length+1]\n",
        "\n",
        "def beam_summarize(model: torch.nn.Module, tokenizer, text: str, device, num_beams: int = 5):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        text_encodings = tokenizer.batch_encode_plus([text], padding=True)\n",
        "        text_ids = torch.tensor(text_encodings.get('input_ids'))\n",
        "        num_tokens = text_ids.shape[1]\n",
        "        text_mask = create_padding_mask(text_ids, 1, device)\n",
        "\n",
        "        summary_tokens = beam(\n",
        "            model,  text_ids, text_mask, max_seq_len=int(num_tokens*0.8), start_token_id=0, end_token_id=2 ,num_beams=num_beams, device=device).flatten()\n",
        "        summary = tokenizer.decode(summary_tokens.tolist()).replace('<s>','').replace('</s>','').replace('<unk>','')\n",
        "        return summary"
      ],
      "metadata": {
        "id": "yL3GffW-7aZ8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer and Loss function"
      ],
      "metadata": {
        "id": "nQmCSKJog8Ib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomOptimizer():\n",
        "    def __init__(self, optimizer, model_dim, num_warmup_steps):\n",
        "        self.optimizer = optimizer\n",
        "        self.model_dim = model_dim\n",
        "        self.num_warmup_steps = num_warmup_steps\n",
        "        self.num_steps = 0\n",
        "\n",
        "    def state_dict(self):\n",
        "        optimizer_state_dict = {\n",
        "            'model_dim':self.model_dim,\n",
        "            'num_warmup_steps':self.num_warmup_steps,\n",
        "            'num_steps':self.num_steps,\n",
        "            'optimizer':self.optimizer.state_dict(),\n",
        "        }\n",
        "        return optimizer_state_dict\n",
        "    \n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.model_dim = state_dict['model_dim']\n",
        "        self.num_warmup_steps = state_dict['num_warmup_steps']\n",
        "        self.num_steps = state_dict['num_steps']\n",
        "        self.optimizer.load_state_dict(state_dict['optimizer'])\n",
        "\n",
        "    def step(self):\n",
        "        self._update_learning_rate()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "    def _update_learning_rate(self):\n",
        "        self.num_steps = self.num_steps + 1\n",
        "        lr_scale_1 = self.num_steps ** (-0.5)\n",
        "        lr_scale_2 = self.num_steps * self.num_warmup_steps ** (-1.5)\n",
        "        learning_rate = (self.model_dim ** -0.5) * min(lr_scale_1, lr_scale_2)\n",
        "\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = learning_rate"
      ],
      "metadata": {
        "id": "9imRA_IhJM3V"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, vocab_size, padding_idx, smoothing_score=0.0):\n",
        "        super().__init__()\n",
        "        self.padding_idx = padding_idx\n",
        "        self.smoothing_score = smoothing_score\n",
        "        self.label_score = 1.0 - smoothing_score\n",
        "        self.vocab_size = vocab_size\n",
        "    \n",
        "    def forward(self, model_predicts, real_sequences):\n",
        "        \"\"\" \n",
        "            model_predict: batch_size x target_length x vocab_size\n",
        "            real_sequences: batch_size x target_length\n",
        "\n",
        "        \"\"\"\n",
        "        # real_sequences = real_sequences.view(-1)\n",
        "        # model_predicts = model_predicts.view(-1, self.vocab_size)\n",
        "\n",
        "        pred_distribution = model_predicts.log_softmax(dim=-1)\n",
        "        with torch.no_grad():\n",
        "            true_distribution = torch.zeros_like(pred_distribution)\n",
        "            true_distribution.fill_(self.smoothing_score / (self.vocab_size - 2))\n",
        "            true_distribution.scatter_(1, real_sequences.unsqueeze(1), self.label_score)\n",
        "            true_distribution[:, self.padding_idx] = 0.0\n",
        "            padding_mask = torch.nonzero(real_sequences.data == self.padding_idx, as_tuple=False)\n",
        "            if padding_mask.dim() > 0:\n",
        "                true_distribution.index_fill_(0, padding_mask.squeeze(), 0.0)\n",
        "\n",
        "        loss_for_idx = torch.sum(-true_distribution * pred_distribution, dim=-1)\n",
        "        loss = torch.mean(loss_for_idx)\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "RLad6ocqrMPi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Validation Class"
      ],
      "metadata": {
        "id": "iWMldlBs2z60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/drive/MyDrive/'"
      ],
      "metadata": {
        "id": "-HPcJHFts_7I"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "class Trainer():\n",
        "    def __init__(self, model, num_epochs, optimizer, train_iter, valid_iter, loss_function, device, padding_idx):\n",
        "        self.model = model.to(device)\n",
        "        self.num_epochs = num_epochs\n",
        "        self.optimizer = optimizer\n",
        "        self.train_iter = train_iter\n",
        "        self.valid_iter = valid_iter\n",
        "        self.loss_function = loss_function\n",
        "        self.padding_idx = padding_idx\n",
        "        self.device = device\n",
        "    \n",
        "    def _train_epoch(self):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        train_dataloader = DataLoader(self.train_iter, batch_size=2, collate_fn=collate_fn, num_workers=0, shuffle=True)\n",
        "        count = 0\n",
        "        for text, summary in tqdm(train_dataloader, desc='Training'):\n",
        "            text_input = text.to(self.device) \n",
        "            summary = summary.to(self.device)\n",
        "            summary_input = summary[:, :-1]\n",
        "\n",
        "            text_mask, summary_mask = create_mask(text_input, summary_input, self.padding_idx, self.device)\n",
        "\n",
        "            summary_predict = self.model(text_input, summary_input, text_mask, summary_mask)\n",
        "            summary_real = summary[:, 1:].contiguous()\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss = self.loss_function(summary_predict.reshape(-1, summary_predict.shape[-1]), summary_real.reshape(-1))\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            count = count + 1\n",
        "            if count % 200 == 0:\n",
        "                print(total_loss/count)\n",
        "                torch.save(self.model.state_dict(), save_path + \"model.pt\")\n",
        "                torch.save(self.optimizer.state_dict(), save_path + \"optimizer.pt\")\n",
        "            \n",
        "            del summary_input, text_mask, summary_mask, summary_predict, summary_real, text_input, summary, text \n",
        "            gc.collect()\n",
        "\n",
        "        return total_loss/len(train_dataloader)        \n",
        "\n",
        "    def _validate_epoch(self):\n",
        "        self.model.eval()\n",
        "        valid_dataloader = DataLoader(self.valid_iter, batch_size=2, collate_fn=collate_fn, num_workers=0, shuffle=True)\n",
        "        with torch.no_grad():\n",
        "            total_loss = 0\n",
        "            for text, summary in tqdm(valid_dataloader, desc='Validation'):\n",
        "                text_input = text.to(self.device) \n",
        "                summary = summary.to(self.device)\n",
        "                summary_input = summary[:, :-1]\n",
        "\n",
        "                text_mask, summary_mask = create_mask(text_input, summary_input, self.padding_idx, self.device)\n",
        "\n",
        "                summary_predict = self.model(text_input, summary_input, text_mask, summary_mask)\n",
        "                summary_real = summary[:, 1:].contiguous()\n",
        "\n",
        "                loss = self.loss_function(summary_predict.reshape(-1, summary_predict.shape[-1]), summary_real.reshape(-1))\n",
        "                total_loss +=  loss.item()\n",
        "\n",
        "                del summary_input, text_mask, summary_mask, summary_predict, summary_real, text_input, summary, text \n",
        "                gc.collect()\n",
        "\n",
        "        return total_loss/len(valid_dataloader)\n",
        "\n",
        "    def train_model(self):\n",
        "        for epoch in range(1,self.num_epochs+1):\n",
        "            start_time = time.time()\n",
        "            train_loss = self._train_epoch()\n",
        "            end_time = time.time()\n",
        "            torch.save(self.model.state_dict(), save_path + \"model.pt\")\n",
        "            torch.save(self.optimizer.state_dict(), save_path + \"optimizer.pt\")\n",
        "            val_loss = self._validate_epoch()\n",
        "            print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "        return self.model\n"
      ],
      "metadata": {
        "id": "gIhPJZbcI8xK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model"
      ],
      "metadata": {
        "id": "sVLJliaS-U3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "    text_batch, summary_batch = [], []\n",
        "    for text_sample, summary_sample in batch:\n",
        "        text_batch.append(text_sample.rstrip(\"\\n\"))\n",
        "        summary_batch.append(summary_sample.rstrip(\"\\n\"))\n",
        "    text_encodings = tokenizer.batch_encode_plus(text_batch, padding=True)\n",
        "    text_ids = torch.tensor(text_encodings.get('input_ids'))\n",
        "    \n",
        "    summary_encodings = tokenizer.batch_encode_plus(summary_batch, padding=True)\n",
        "    summary_ids = torch.tensor(summary_encodings.get('input_ids'))\n",
        "\n",
        "    return text_ids, summary_ids"
      ],
      "metadata": {
        "id": "VmjLWkBueWgy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set= pd.read_csv('/content/drive/MyDrive/Train_set_short.csv')\n",
        "train_iter = list(zip(train_set.article, train_set.highlights))\n",
        "validation_set= pd.read_csv('/content/drive/MyDrive/Validation_set_short.csv')\n",
        "validation_iter = list(zip(validation_set.article, validation_set.highlights))"
      ],
      "metadata": {
        "id": "QXUICJldA17Q"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = transformer.to(tran_conf.device)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=1, label_smoothing=0.1)\n",
        "optimizer = CustomOptimizer(torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9), tran_conf.model_dim, 4000)\n",
        "trainer = Trainer(transformer, 1, optimizer, train_iter, validation_iter, loss_fn, tran_conf.device, 1)"
      ],
      "metadata": {
        "id": "7Rb8MEJ588PO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train_model()"
      ],
      "metadata": {
        "id": "ZfLYb9HqA2A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continue training if lacking of training time"
      ],
      "metadata": {
        "id": "0rDcGAuPO38g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Transformer.utils.beamsearch import beam_summarize\n",
        "from Transformer.model.transformer_model import Transformer\n",
        "from Transformer.model.transformer_config import TransformerConfig\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "import torch\n",
        "\n",
        "kwargs = {\n",
        "    'vocab_size': 50264,\n",
        "    'max_position_embeddings': 1024,\n",
        "    'num_encoder_layers': 12,\n",
        "    'encoder_ffn_dim': 4096,\n",
        "    'encoder_attention_heads': 16,\n",
        "    'num_decoder_layers': 12,\n",
        "    'decoder_ffn_dim': 4096,\n",
        "    'decoder_attention_heads': 16,\n",
        "    'encoder_layer_dropout': 0.0,\n",
        "    'decoder_layer_dropout': 0.0,\n",
        "    'activation_function': 'gelu',\n",
        "    'layer_norm_eps': 1e-5,\n",
        "    'model_dim': 1024,\n",
        "    'dropout': 0.1,\n",
        "    'attention_dropout': 0.0,\n",
        "    'activation_dropout': 0.0,\n",
        "    'pad_token_id': 1,\n",
        "    'device': 'cuda',\n",
        "    'dtype': torch.float32\n",
        "}\n",
        "tran_conf = TransformerConfig(**kwargs)\n",
        "\n",
        "transformer = Transformer(tran_conf)\n",
        "\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "transformer.load_state_dict(torch.load('/content/drive/MyDrive/final_model.pt'))"
      ],
      "metadata": {
        "id": "EB63FIgM2Wwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set= pd.read_csv('/content/drive/MyDrive/Train_set_short.csv')\n",
        "train_iter = list(zip(train_set.article, train_set.highlights))\n",
        "validation_set= pd.read_csv('/content/drive/MyDrive/Validation_set_short.csv')\n",
        "validation_iter = list(zip(validation_set.article, validation_set.highlights))  "
      ],
      "metadata": {
        "id": "GragbXE2PL1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = transformer.to(tran_conf.device)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=1, label_smoothing=0.1)\n",
        "optimizer = CustomOptimizer(torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9), tran_conf.model_dim, 4000)\n",
        "trainer = Trainer(transformer, 1, optimizer, train_iter, validation_iter, loss_fn, tran_conf.device, 1)"
      ],
      "metadata": {
        "id": "uni2UxwLPoIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train_model()"
      ],
      "metadata": {
        "id": "frbEyro9RM05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "L2XFMFpLd2_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\"Istanbul (CNN)A woman carried out a suicide bombing at a police station in Istanbul's historic Sultanahmet district Tuesday evening, killing one police officer and injuring another, officials said. The attack happened in the section of Turkey's largest city that is home to landmarks such as the Hagia Sophia and the Blue Mosque, and is heavily trafficked by tourists. The bomber, speaking English, entered the police station saying she lost her wallet, and the explosion happened at about 5:20 p.m., Istanbul Gov. Vasip Sahin told reporters. Sahin did not mention a motive for the attack. Sahin initially said that the blast, besides killing the bomber, critically injured one police officer and slightly wounded another. Later Tuesday, Turkey's semi-official Anadolu news agency reported that one of the officers died of his wounds at a hospital. Police cordoned off the area. The attacker's identity is unknown and the incident is being investigated, the governor told reporters. CNN's Gul Tuysuz reported and wrote from Istanbul, and CNN's Jason Hanna wrote in Atlanta. CNN's Hande Atay contributed to this report.?\"\"\""
      ],
      "metadata": {
        "id": "_A-QYETLRvP6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Scientists say weird signals from space are 'probably' aliens. A team of astronomers believes that strange signals emanating from a cluster of stars are actually aliens trying to tell the universe they exist. The study, which appeared in the Publications of the Astronomical Society of the Pacific, analyzed the odd beams of light from 234 stars - a fraction of the 2.5 million that were observed. The bizarre beacons led the paper's authors, Ermanno F. Borra and Eric Trottier from Laval University in Quebec, to conclude that it's \"probably\" aliens. \"We find that the detected signals have exactly the shape of an [extraterrestrial intelligence] signal predicted in the previous publication and are therefore in agreement with this hypothesis,\" wrote Borra and Trottier. They also note that their findings align with the Extraterrestrial Intelligence (ETI) hypothesis, since the mysterious activity only occurred in a tiny fraction of stars. The hypothesis also suggests that an intelligent life force would use a more sophisticated optical beacon than, say, radio waves to reveal its existence.\"\"\""
      ],
      "metadata": {
        "id": "uAILGp6CQ-t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Despite the positive news for tennis star Novak Djokovic on Monday, whether he will be able to compete in the Australian Open later this month still remains unclear. If Djokovic is allowed to stay, when will he play? Following his release from detention, the Serbian tennis star has returned to training, according to his brother. Djokovic has made clear in a series of tweets that he still intends to play in the tournament. We don't know yet when Djokovic's first match would be, but the main draw is on is Thursday January 13.\"\"\""
      ],
      "metadata": {
        "id": "zecfPcEFRAYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = beam_summarize(transformer, tokenizer, text, device='cuda')\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "WTt7_vpj0AoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "4ToFq2zEynXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "id": "8ZJ56bHqyJag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from rouge import Rouge \n",
        "\n",
        "\n",
        "def rouge(test_path, model, beam_search=True, num_beams=3, return_sentence=False):\n",
        "    model.eval()\n",
        "    data_set= pd.read_csv(test_path) \n",
        "    data_iter = list(zip(data_set.article, data_set.highlights))[0:50]\n",
        "    rouge = Rouge()\n",
        "\n",
        "    total_score = 0\n",
        "    pred_texts = []\n",
        "    tgt_texts = []\n",
        "\n",
        "    for src, tgt in tqdm(data_iter, desc='Rouge score'):\n",
        "        pred_tgt = beam_summarize(model, src, num_beams=num_beams)\n",
        "        scores = rouge.get_scores(pred_tgt, tgt)\n",
        "        print(scores)"
      ],
      "metadata": {
        "id": "02bXWVSbszV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = rouge('/content/drive/MyDrive/Test_set_short.csv', transformer, True, 4, False)"
      ],
      "metadata": {
        "id": "zEQTvs5Sx-k7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}